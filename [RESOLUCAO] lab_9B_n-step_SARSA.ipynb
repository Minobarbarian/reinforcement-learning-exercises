{"cells":[{"cell_type":"markdown","source":["# IMD1103 - Aprendizado por Reforço"],"metadata":{"id":"WcQHz2tlF51I"}},{"cell_type":"markdown","source":["### Professor: Dr. Leonardo Enzo Brito da Silva"],"metadata":{"id":"sOZzae8SF51I"}},{"cell_type":"markdown","source":["### Aluno: João Antonio Costa Paiva Chagas"],"metadata":{"id":"egyBxOL-F51J"}},{"cell_type":"markdown","metadata":{"id":"hf0af9vLqLW3"},"source":["# Laboratório 9B: n-step Sarsa (CliffWalking)"]},{"cell_type":"markdown","metadata":{"id":"f3ahVih93jpQ"},"source":["## Importações"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtRypzNFEb3t"},"outputs":[],"source":["# Instala os pacotes necessários:\n","# - gymnasium[toy-text]: inclui ambientes simples como FrozenLake, Taxi, etc.\n","# - imageio[ffmpeg]: permite salvar vídeos e GIFs (formato .mp4 ou .gif)\n","!pip install gymnasium[toy-text]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FTh_QK47EfwM"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import gymnasium as gym\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from typing import Dict, Tuple, List, Optional\n","from matplotlib.collections import LineCollection"]},{"cell_type":"markdown","source":["## Armazenamento"],"metadata":{"id":"ItBdrnoWTFNc"}},{"cell_type":"code","source":["output_dir = \"resultados_plots\"\n","os.makedirs(output_dir, exist_ok=True)"],"metadata":{"id":"_QmK9F3GTE_g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eLfMlW3VqLW5"},"source":["## Funções auxiliares para visualização"]},{"cell_type":"code","source":["def plotar_comparacao_metricas(\n","    results: Dict[str, Dict[str, list]],\n","    titulo_prefixo: str,\n","    janela: int = 100,\n","    save_path: Optional[str] = None\n",") -> None:\n","    \"\"\"\n","    Plota a comparação de métricas (tamanho e retorno) de múltiplos\n","    experimentos em uma única figura.\n","    \"\"\"\n","    fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12, 8), sharex=True)\n","    sns.set_palette(\"viridis\", n_colors=len(results)) # Optional: color palette\n","\n","    # Plot 1 – Tamanho do episódio\n","    for label, data in results.items():\n","        df = pd.DataFrame({'tamanho': data['T']})\n","        tamanho_ma = df['tamanho'].rolling(window=janela).mean()\n","        sns.lineplot(x=tamanho_ma.index, y=tamanho_ma, ax=axs[0], label=label)\n","\n","    axs[0].set_title(f'{titulo_prefixo} - Tamanho do Episódio')\n","    axs[0].set_ylabel(f'Passos por Episódio (Média Móvel {janela})')\n","    axs[0].legend()\n","    axs[0].grid()\n","\n","    # Plot 2 – Retorno\n","    for label, data in results.items():\n","        df = pd.DataFrame({'retorno': data['G']})\n","        retorno_ma = df['retorno'].rolling(window=janela).mean()\n","        sns.lineplot(x=retorno_ma.index, y=retorno_ma, ax=axs[1], label=label)\n","\n","    axs[1].set_title(f'{titulo_prefixo} - Retorno do Episódio')\n","    axs[1].set_xlabel('Episódio')\n","    axs[1].set_ylabel(f'Recompensa Total (Média Móvel {janela})')\n","    axs[1].legend()\n","    axs[1].grid()\n","\n","    plt.tight_layout()\n","    if save_path:\n","        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n","        plt.close(fig)\n","        print(f\"Plot de comparação salvo em: {save_path}\")\n","    else:\n","        plt.show()"],"metadata":{"id":"DnRwVcu2HGKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plotar_metricas_single(\n","    episodio_len: list[int],\n","    episodio_return: list[float],\n","    titulo: str,\n","    janela: int = 100,\n","    save_path: Optional[str] = None\n",") -> None:\n","    \"\"\"\n","    Usa Pandas + Seaborn para plotar as métricas de uma única execução.\n","    \"\"\"\n","    df = pd.DataFrame({\n","        'episodio': np.arange(len(episodio_len)),\n","        'tamanho': episodio_len,\n","        'retorno': episodio_return\n","    })\n","\n","    df['tamanho_ma'] = df['tamanho'].rolling(window=janela).mean()\n","    df['retorno_ma'] = df['retorno'].rolling(window=janela).mean()\n","\n","    fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12, 8), sharex=True)\n","    fig.suptitle(titulo, fontsize=16)\n","\n","    # Plot 1 – Tamanho do episódio\n","    sns.lineplot(data=df, x='episodio', y='tamanho', ax=axs[0], label='Tamanho do Episódio', alpha=0.3)\n","    sns.lineplot(data=df, x='episodio', y='tamanho_ma', ax=axs[0], label=f'Média móvel ({janela})')\n","    axs[0].set_ylabel('Passos por Episódio')\n","    axs[0].legend()\n","    axs[0].grid()\n","\n","    # Plot 2 – Retorno\n","    sns.lineplot(data=df, x='episodio', y='retorno', ax=axs[1], label='Retorno do Episódio', color='orange', alpha=0.3)\n","    sns.lineplot(data=df, x='episodio', y='retorno_ma', ax=axs[1], label=f'Média móvel ({janela})', color='red')\n","    axs[1].set_xlabel('Episódio')\n","    axs[1].set_ylabel('Recompensa Total')\n","    axs[1].legend()\n","    axs[1].grid()\n","\n","    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make room for suptitle\n","    if save_path:\n","        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n","        plt.close(fig)\n","        print(f\"Plot de métricas salvo em: {save_path}\")\n","    else:\n","        plt.show()"],"metadata":{"id":"P-gzy6ksVrNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# helpers de grid/máscaras\n","def _grid_info_from_env(env_name: str, map_name: str | None = None, is_slippery: bool = False):\n","    \"\"\"\n","    Retorna (n_rows, n_cols, holes_set, cliffs_set, goals_set, start_rc)\n","    para FrozenLake-v1 ou CliffWalking-v1.\n","    \"\"\"\n","    holes, cliffs, goals = set(), set(), set()\n","    start_rc = None\n","\n","    if env_name == \"FrozenLake-v1\":\n","        kwargs = {}\n","        if map_name is not None:\n","            kwargs[\"map_name\"] = map_name\n","        kwargs[\"is_slippery\"] = is_slippery\n","\n","        env = gym.make(\"FrozenLake-v1\", **kwargs)\n","        desc = env.unwrapped.desc\n","        # decode se dtype for bytes\n","        desc = np.array(desc, dtype=str) if desc.dtype.kind != \"S\" else np.char.decode(desc, \"utf-8\")\n","        n_rows, n_cols = desc.shape\n","        for r in range(n_rows):\n","            for c in range(n_cols):\n","                ch = desc[r, c]\n","                if ch == \"H\": holes.add((r, c))\n","                elif ch == \"G\": goals.add((r, c))\n","                elif ch == \"S\": start_rc = (r, c)\n","        if start_rc is None:\n","            start_rc = (0, 0)\n","        env.close()\n","        return n_rows, n_cols, holes, cliffs, goals, start_rc\n","\n","    elif env_name == \"CliffWalking-v1\":\n","        n_rows, n_cols = 4, 12\n","        start_rc = (n_rows - 1, 0)\n","        goal_rc  = (n_rows - 1, n_cols - 1)\n","        goals.add(goal_rc)\n","        # penhasco: última linha, colunas 1..10\n","        for c in range(1, n_cols - 1):\n","            cliffs.add((n_rows - 1, c))\n","        return n_rows, n_cols, holes, cliffs, goals, start_rc\n","\n","    else:\n","        raise ValueError(\"env_name deve ser 'FrozenLake-v1' ou 'CliffWalking-v1'.\")\n","\n","def _rc_from_state(s: int, n_cols: int) -> tuple[int, int]:\n","    return divmod(s, n_cols)  # (r,c)\n","\n","# simulação de trajetória\n","def simular_trajetoria_gym(\n","    Pi: np.ndarray,\n","    env_name: str,\n","    *,\n","    map_name: str | None = None,\n","    is_slippery: bool = False,\n","    max_steps: int = 200\n","):\n","    \"\"\"\n","    Executa uma trajetória determinística (gulosa em Pi) no Gym (FrozenLake/CliffWalking).\n","    Retorna:\n","      - estados: lista de (r,c)\n","      - acoes: lista de ints\n","      - recompensas: lista de floats\n","    \"\"\"\n","    # cria env\n","    if env_name == \"FrozenLake-v1\":\n","        kwargs = {}\n","        if map_name is not None:\n","            kwargs[\"map_name\"] = map_name\n","        kwargs[\"is_slippery\"] = is_slippery\n","        env = gym.make(\"FrozenLake-v1\", **kwargs)\n","    elif env_name == \"CliffWalking-v1\":\n","        env = gym.make(\"CliffWalking-v1\")\n","    else:\n","        raise ValueError(\"env_name deve ser 'FrozenLake-v1' ou 'CliffWalking-v1'.\")\n","\n","    n_rows, n_cols, holes, cliffs, goals, start_rc = _grid_info_from_env(env_name, map_name, is_slippery)\n","\n","    # sanity check de dimensões\n","    n_states, n_actions = Pi.shape\n","    assert n_states == n_rows * n_cols, f\"Pi.shape[0]={n_states} != n_rows*n_cols={n_rows*n_cols}\"\n","    assert n_actions == env.action_space.n, f\"Pi.shape[1]={n_actions} != action_space.n={env.action_space.n}\"\n","\n","    state, _ = env.reset()\n","    # se o env permitir setar estado diretamente, tenta (FrozenLake/cliff costumam expor .unwrapped.s)\n","    if hasattr(env.unwrapped, \"s\"):\n","        env.unwrapped.s = int(state)\n","\n","    estados_rc = [_rc_from_state(int(state), n_cols)]\n","    acoes, recompensas = [], []\n","\n","    for _ in range(max_steps):\n","        s = int(state)\n","        a = int(np.argmax(Pi[s]))\n","        next_state, reward, terminated, truncated, _ = env.step(a)\n","\n","        acoes.append(a)\n","        recompensas.append(float(reward))\n","        estados_rc.append(_rc_from_state(int(next_state), n_cols))\n","\n","        state = next_state\n","        if terminated or truncated:\n","            break\n","\n","    env.close()\n","    return estados_rc, acoes, recompensas"],"metadata":{"id":"DABfiX7kK2ER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --------------------------\n","# plot da trajetória no grid\n","# --------------------------\n","def plot_trajetoria_gym(\n","    env_name: str,\n","    estados: list[tuple[int, int]],\n","    *,\n","    map_name: str | None = None,\n","    is_slippery: bool = False,\n","    titulo: str = \"Trajetória gulosa (Gym)\",\n","    gradiente_temporal: bool = True,\n","    mostrar_setas: bool = True,\n","    save_path: Optional[str] = None\n","):\n","    \"\"\"\n","    Plota a trajetória sobre um grid para FrozenLake/CliffWalking com destaques de buracos/penhasco/goal.\n","    \"\"\"\n","    n_rows, n_cols, holes, cliffs, goals, start_rc = _grid_info_from_env(env_name, map_name, is_slippery)\n","\n","    # figura/base do grid\n","    fig, ax = plt.subplots(figsize=(n_cols, n_rows))\n","    ax.set_xlim(0, n_cols); ax.set_ylim(0, n_rows)\n","    ax.set_xticks(np.arange(0, n_cols + 1, 1))\n","    ax.set_yticks(np.arange(0, n_rows + 1, 1))\n","    ax.grid(True); ax.set_aspect('equal'); ax.invert_yaxis()\n","\n","    # células coloridas\n","    for r in range(n_rows):\n","        for c in range(n_cols):\n","            cell = (r, c)\n","            if env_name == \"FrozenLake-v1\":\n","                if cell in holes:\n","                    color = (1.0, 0.0, 0.0, 0.25)  # vermelho translúcido\n","                elif cell in goals:\n","                    color = (0.0, 1.0, 0.0, 0.25)  # verde translúcido\n","                elif cell == start_rc:\n","                    color = (1.0, 1.0, 0.0, 0.18)  # amarelo leve\n","                else:\n","                    color = 'white'\n","            else:  # CliffWalking\n","                if cell in cliffs:\n","                    color = (1.0, 0.0, 0.0, 0.25)\n","                elif cell in goals:\n","                    color = (0.0, 1.0, 0.0, 0.25)\n","                elif cell == start_rc:\n","                    color = (1.0, 1.0, 0.0, 0.18)\n","                else:\n","                    color = 'white'\n","\n","            rect = patches.Rectangle((c, r), 1, 1, facecolor=color, edgecolor='gray')\n","            ax.add_patch(rect)\n","\n","    # extrai xs, ys (centros)\n","    xs = [c + 0.5 for (_, c) in estados]\n","    ys = [r + 0.5 for (r, _) in estados]\n","\n","    # linha com gradiente temporal\n","    if gradiente_temporal and len(xs) > 1:\n","        pontos = np.array([xs, ys]).T\n","        segmentos = np.stack([pontos[:-1], pontos[1:]], axis=1)\n","        lc = LineCollection(segmentos, linewidths=2.5)\n","        cores = np.linspace(0, 1, len(segmentos))\n","        lc.set_array(cores)\n","        ax.add_collection(lc)\n","        cbar = plt.colorbar(lc, ax=ax, fraction=0.046, pad=0.04)\n","        cbar.set_label(\"Progresso temporal\")\n","    else:\n","        ax.plot(xs, ys, '-o', linewidth=2.5, markersize=4)\n","\n","    # início/fim\n","    ax.scatter(xs[0], ys[0], marker='*', s=220, edgecolor='black', facecolor='yellow', zorder=5, linewidths=1.2, label='Início')\n","    ax.scatter(xs[-1], ys[-1], marker='o', s=150, edgecolor='black', facecolor='none', zorder=5, linewidths=1.2, label='Fim')\n","\n","    # setas entre células\n","    if mostrar_setas:\n","        for i in range(len(xs) - 1):\n","            dx, dy = xs[i+1] - xs[i], ys[i+1] - ys[i]\n","            ax.arrow(xs[i], ys[i], dx*0.85, dy*0.85, head_width=0.15, head_length=0.15,\n","                     length_includes_head=True, fc='black', ec='black', alpha=0.8)\n","\n","    ax.set_title(titulo)\n","    ax.legend(loc='upper right', frameon=True)\n","    plt.tight_layout()\n","    if save_path:\n","        directory = os.path.dirname(save_path)\n","        if directory:\n","            os.makedirs(directory, exist_ok=True)\n","        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n","        plt.close(fig)\n","        print(f\"Plot da política salvo em: {save_path}\")\n","    else:\n","        plt.show()"],"metadata":{"id":"6PpD6ldhF5C9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AOeAMZ4ux_GJ"},"source":["## Algoritmo: n-step Sarsa"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hHk59STi_puz"},"outputs":[],"source":["# Alvo n passos\n","def _alvo_n_passos(S, A, R, tau: int, n: int, T_end: int, gamma: float, Q: np.ndarray) -> float:\n","    Gtau = 0.0\n","    upper_sum = int(min(tau + n, T_end))        # limite superior do somatório\n","    lower_sum = tau + 1                         # limite inferior do somatório\n","    for i in range(lower_sum, upper_sum + 1):\n","        Gtau += (gamma ** (i - tau - 1)) * R[i]\n","    if (tau + n) < T_end:\n","        Gtau += (gamma ** n) * Q[S[tau + n], A[tau + n]]\n","    return Gtau\n","\n","# Atualização dos valores de ação (n-step SARSA)\n","def _atualiza_Q(Q, experiencia, tau, n, T_end, alpha, gamma):\n","    S, A, R = experiencia\n","    td_target = _alvo_n_passos(S, A, R, tau, n, T_end, gamma, Q)\n","    td_error  = Q[S[tau], A[tau]] - td_target\n","    Q[S[tau], A[tau]] -= alpha * td_error\n","\n","# Atualização da política\n","def _atualiza_Pi(Pi: np.ndarray, Q: np.ndarray, s: int, eps: float) -> None:\n","    \"\"\"\n","    Deixa Pi[s, :] ε-suave em torno de argmax_a Q[s, a].\n","    \"\"\"\n","    n_acoes = Q.shape[1]\n","    a_star = int(np.argmax(Q[s]))\n","    Pi[s, :] = eps / n_acoes\n","    Pi[s, a_star] += 1.0 - eps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OT0sAi8z_puz"},"outputs":[],"source":["def n_step_sarsa(\n","    env: gym.Env,\n","    n: int = 3,\n","    gamma: float = 0.9,\n","    N: int = 500,\n","    T: int = 1_000,\n","    epsilon: float = 0.1,\n","    alpha: float = 0.1,\n","    seed: Optional[int] = None,\n",") -> Tuple[np.ndarray, np.ndarray, np.ndarray, int, List[int], List[float]]:\n","    \"\"\"\n","    n-step SARSA  com política ε-gulosa.\n","\n","    O algoritmo mantém buffers de sequência (S, A, R) e atualiza Q(S_τ, A_τ) usando o retorno n-passos:\n","        G_τ = sum_{i=τ+1}^{min(τ+n, T_end)} γ^{i-τ-1} R_i  +  1_{τ+n < T_end} · γ^n · Q(S_{τ+n}, A_{τ+n})\n","    Para n = 1, recupera-se o SARSA(0).\n","\n","    Parâmetros\n","    ----------\n","    env : gymnasium.Env\n","        Ambiente do gymnasium (ex.: 'CliffWalking-v1', 'FrozenLake-v1').\n","    n : int\n","        Horizonte do retorno n-passos (n ≥ 1). Para n=1, equivale ao SARSA(0).\n","    gamma : float\n","        Fator de desconto (0 ≤ γ ≤ 1).\n","    N : int\n","        Número de episódios de treinamento.\n","    T : int\n","        Limite máximo de passos por episódio.\n","    epsilon : float\n","        Parâmetro ε da política ε-gulosa (0 ≤ ε ≤ 1) utilizada on-policy.\n","    alpha : float\n","        Taxa de aprendizado (0 < α ≤ 1).\n","    seed : int | None\n","        Semente de aleatoriedade.\n","\n","    Retorna\n","    -------\n","    Q : np.ndarray, shape (n_states, n_actions)\n","        Estimativas finais dos valores de ação Q(s, a).\n","    Pi : np.ndarray, shape (n_states, n_actions)\n","        Política ε-suave final (linhas somam ≈ 1).\n","    numero_de_visitas : np.ndarray, shape (n_states, n_actions)\n","        Contagem de visitas a cada par (s, a) durante o treinamento.\n","    k : int\n","        Número de episódios efetivamente executados (== N).\n","    episodio_T : list[int]\n","        Duração (passos) de cada episódio, considerando término natural ou truncação em T.\n","    episodio_G : list[float]\n","        Retorno não-descontado (soma de recompensas) obtido em cada episódio.\n","    \"\"\"\n","\n","    rng = np.random.default_rng(seed)\n","\n","    # Listas de métricas\n","    episodio_T = []\n","    episodio_G = []\n","\n","    # Atalhos\n","    n_states  = env.observation_space.n\n","    n_actions = env.action_space.n\n","\n","    # Inicializações\n","    Q                 = np.zeros((n_states, n_actions), dtype=float)\n","    numero_de_visitas = np.zeros((n_states, n_actions), dtype=float)\n","\n","    # Política inicial ε-suave: uniforme\n","    Pi = np.full((n_states, n_actions), 1.0 / n_actions, dtype=float)\n","\n","    # Loop episódios\n","    for _ in tqdm(range(1, N + 1), desc=f\"Episódios (n-step SARSA, n={n})\", leave=True):\n","\n","        # Reset do ambiente\n","        state, _ = env.reset()\n","\n","        ############################################################################\n","        # Implementação aqui\n","        # Dica:\n","        # usar\n","        # próximo estado, recompensa, terminated (flag), truncated (flag), _ = env.step(ação)\n","        # para fazer a trasição de um estado para o outro dada uma ação do agente\n","\n","        S = np.zeros(T + 1, dtype=int)\n","        A = np.zeros(T + 1, dtype=int)\n","        R = np.zeros(T + 1, dtype=float)\n","\n","        S[0] = int(state)\n","        A[0] = int(rng.choice(n_actions, p=Pi[S[0]]))\n","\n","        t = 0\n","        G = 0.0\n","        T_end = np.inf\n","\n","        while True:\n","            if t < T_end:\n","                if t >= T:\n","                    T_end = T\n","                else:\n","                    next_state, reward, terminated, truncated, _ = env.step(A[t])\n","\n","                    S[t + 1] = int(next_state)\n","                    R[t + 1] = reward\n","\n","                    G += reward\n","                    numero_de_visitas[S[t], A[t]] += 1.0\n","\n","                    if terminated or truncated:\n","                        T_end = t + 1\n","                    else:\n","                        A[t + 1] = int(rng.choice(n_actions, p=Pi[S[t + 1]]))\n","\n","            tau = t - n + 1\n","\n","            if tau == T_end:\n","                break\n","\n","\n","            if tau >= 0:\n","                experiencia = (S, A, R)\n","\n","                _atualiza_Q(Q, experiencia, tau, n, T_end, alpha, gamma)\n","\n","                _atualiza_Pi(Pi, Q, S[tau], epsilon)\n","\n","            t += 1\n","\n","        episodio_T.append(T_end)\n","        episodio_G.append(G)\n","\n","        ############################################################################\n","\n","    return Q, Pi, numero_de_visitas, N, episodio_T, episodio_G"]},{"cell_type":"markdown","metadata":{"id":"P2PS4lRgqLW9"},"source":["## Experimento"]},{"cell_type":"code","source":["def executar_experimento(\n","    nome_experimento: str,\n","    ambiente: str,\n","    algoritmo: callable,\n","    **kwargs\n",") -> Tuple[np.ndarray, List[int], List[float]]:\n","    \"\"\"\n","    Executa o algoritmo e retorna a política final, T e G.\n","    \"\"\"\n","    print(f\"--- Executando: {nome_experimento} ---\")\n","    env = gym.make(ambiente)\n","    try:\n","        _, Pi, _, _, T, G = algoritmo(env=env, **kwargs)\n","        return Pi, T, G\n","    finally:\n","        env.close()"],"metadata":{"id":"c-jaaL9iGmQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def executar_variacao_de_hiperparametro(\n","    nome_variacao: str,\n","    param_valores: list,\n","    algoritmo: callable,\n","    ambiente: str,\n","    output_dir: str,\n","    params_fixos: dict\n","):\n","    \"\"\"\n","    Executa variações de um hiperparâmetro,\n","    gerando plots comparativos de métricas e trajetórias individuais.\n","    \"\"\"\n","    print(f\"\\n==============================================\")\n","    print(f\"  Iniciando: Variação de {nome_variacao}\")\n","    print(f\"==============================================\\n\")\n","\n","    results_metricas = {}\n","    param_valores_sorted = sorted(list(param_valores))\n","\n","    # 1. Resultados de todas as execuções\n","    for valor in param_valores_sorted:\n","        nome_exp = f\"{nome_variacao}={valor}\"\n","        params_atuais = params_fixos.copy()\n","        chave_param = {\"NSTEPS\": \"n\", \"ALPHA\": \"alpha\", \"GAMMA\": \"gamma\", \"EPSILON\": \"epsilon\"}[nome_variacao]\n","        params_atuais[chave_param] = valor\n","\n","        Pi, T, G = executar_experimento(\n","            nome_experimento=nome_exp,\n","            ambiente=ambiente,\n","            algoritmo=algoritmo,\n","            **params_atuais\n","        )\n","\n","        results_metricas[nome_exp] = {'T': T, 'G': G}\n","\n","        # 2. Plota a trajetória individual para cada execução\n","        estados, _, _ = simular_trajetoria_gym(Pi, ambiente, max_steps=200)\n","        plot_trajetoria_gym(\n","            ambiente,\n","            estados,\n","            titulo=f\"Trajetória (gulosa) — {nome_exp}\",\n","            save_path=f\"{output_dir}/trajetoria_{nome_exp}.pdf\"\n","        )\n","\n","    # 3. Plota comparação de métricas para a variação do hiperparâmetro\n","    plotar_comparacao_metricas(\n","        results=results_metricas,\n","        titulo_prefixo=f\"Variação de {nome_variacao}\",\n","        save_path=f\"{output_dir}/comparacao_{nome_variacao}.pdf\"\n","    )\n","    print(f\"\\n--- Variação de '{nome_variacao}' concluída. ---\")"],"metadata":{"id":"8xZhlLqdIAxK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"okxUUETkqLW_"},"source":["# Tarefa:\n","\n","1. Implemente o algoritmo n-step Sarsa para resolver o ambiente `'CliffWalking-v1'` do [gymnasium](https://gymnasium.farama.org/environments/toy_text/cliff_walking/).\n","2. Varie o hiperparametro (NSTEPS) e fixe os demais (ex.: ALPHA, GAMMA, EPSILON) (Obs>: 5 valores distintos).\n","    - Para o estudo do hiperparâmetro plote:\n","        - a duração do episódio por episódio\n","        - a recompensa total por episodio\n","        - Uma trajetória gulosa utilizando `plot_trajetoria_gym`.\n","    - Observação: as curvas para cada estudo de hiperparâmetro devem estar na mesma figura.\n","3. Repita o procedimento para cada um dos hiperparametros.\n","4. Reporte suas observações.\n","\n","**Entregáveis:**\n","\n","2. **Código** (notebook `*.ipynb`)\n","1. **Relatório** (`*.pdf`).\n","- O PDF deve conter:\n","  - **Setup** (parâmetros usados).\n","  - **Resultados** (figuras e tabelas organizadas por experimento).\n","  - **Análises curtas** por experimento.\n","- O PDF **NÃO** deve conter:\n","    - Códigos."]},{"cell_type":"markdown","source":["## Hiperparâmetros Fixos"],"metadata":{"id":"2JZMB7-_M6QN"}},{"cell_type":"code","source":["params_fixos = {\n","    \"N\": 2000,\n","    \"n\": 1,\n","    \"alpha\": 0.01,\n","    \"gamma\": 0.9,\n","    \"epsilon\": 0.3,\n","    \"seed\": 42\n","}"],"metadata":{"id":"rYtetBJtKehP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Baseline"],"metadata":{"id":"9FYY-ESaVzxc"}},{"cell_type":"code","source":["print(\"\\n################# EXPERIMENTO BASELINE #################\")\n","Pi_base, T_base, G_base = executar_experimento(\n","    nome_experimento=\"BASELINE\",\n","    ambiente=\"CliffWalking-v1\",\n","    algoritmo=n_step_sarsa,\n","    **params_fixos\n",")\n","\n","plotar_metricas_single(\n","    episodio_len=T_base,\n","    episodio_return=G_base,\n","    titulo=\"Métricas da Execução Baseline\",\n","    save_path=f\"{output_dir}/comparacao_BASELINE.pdf\"\n",")\n","\n","estados_base, _, _ = simular_trajetoria_gym(Pi_base, \"CliffWalking-v1\", max_steps=200)\n","plot_trajetoria_gym(\n","    \"CliffWalking-v1\",\n","    estados_base,\n","    titulo=\"Trajetória (gulosa) — BASELINE\",\n","    save_path=f\"{output_dir}/trajetoria_BASELINE.pdf\"\n",")"],"metadata":{"id":"3ZVdNUM8Vz_P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Experimento 1 - Variação de NSTEPS"],"metadata":{"id":"eZDjQEXeLrNB"}},{"cell_type":"code","source":["print(\"\\n################# EXPERIMENTO 1 #################\")\n","params_variacao_nsteps = {k: v for k, v in params_fixos.items() if k != \"n\"}\n","executar_variacao_de_hiperparametro(\n","    nome_variacao=\"NSTEPS\",\n","    param_valores=[2, 3, 4, 5, 6],\n","    algoritmo=n_step_sarsa,\n","    ambiente=\"CliffWalking-v1\",\n","    output_dir=output_dir,\n","    params_fixos=params_variacao_nsteps\n",")"],"metadata":{"id":"tjBeT1RRLlmj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Experimento 2 - Variação de ALPHA"],"metadata":{"id":"YYJ54aCFLteo"}},{"cell_type":"code","source":["print(\"\\n################# EXPERIMENTO 2 #################\")\n","params_variacao_alpha = {k: v for k, v in params_fixos.items() if k != \"alpha\"}\n","executar_variacao_de_hiperparametro(\n","    nome_variacao=\"ALPHA\",\n","    param_valores=[0.0001, 0.001, 0.05, 0.1],\n","    algoritmo=n_step_sarsa,\n","    ambiente=\"CliffWalking-v1\",\n","    output_dir=output_dir,\n","    params_fixos=params_variacao_alpha\n",")"],"metadata":{"id":"GDXf-HHsLwap"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Experimento 3 - Variação de GAMMA"],"metadata":{"id":"IvAaVx7-Ltz_"}},{"cell_type":"code","source":["print(\"\\n################# EXPERIMENTO 3 #################\")\n","params_variacao_gamma = {k: v for k, v in params_fixos.items() if k != \"gamma\"}\n","executar_variacao_de_hiperparametro(\n","    nome_variacao=\"GAMMA\",\n","    param_valores=[0.5, 0.7, 0.95, 0.99],\n","    algoritmo=n_step_sarsa,\n","    ambiente=\"CliffWalking-v1\",\n","    output_dir=output_dir,\n","    params_fixos=params_variacao_gamma\n",")"],"metadata":{"id":"J52A0N5ELw56"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Experimento 4 - Variação de EPSILON"],"metadata":{"id":"Csk0mApeLuJ4"}},{"cell_type":"code","source":["print(\"\\n################# EXPERIMENTO 4 #################\")\n","params_variacao_epsilon = {k: v for k, v in params_fixos.items() if k != \"epsilon\"}\n","executar_variacao_de_hiperparametro(\n","    nome_variacao=\"EPSILON\",\n","    param_valores=[0.1, 0.2, 0.6, 0.9],\n","    algoritmo=n_step_sarsa,\n","    ambiente=\"CliffWalking-v1\",\n","    output_dir=output_dir,\n","    params_fixos=params_variacao_epsilon\n",")"],"metadata":{"id":"cMif2JSrLxOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r /content/resultados_plots.zip /content/resultados_plots"],"metadata":{"id":"Ou6wX4SYPBqw"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["ItBdrnoWTFNc","eZDjQEXeLrNB","YYJ54aCFLteo","IvAaVx7-Ltz_","Csk0mApeLuJ4"],"provenance":[{"file_id":"1MGvmkLivW1eGUQLlkbJ-FZKq7wLO3p7-","timestamp":1761256851164}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}