{"cells":[{"cell_type":"markdown","source":["# IMD1103 - Aprendizado por Reforço"],"metadata":{"id":"X-XapzRBJR-9"}},{"cell_type":"markdown","source":["### Professor: Dr. Leonardo Enzo Brito da Silva"],"metadata":{"id":"zZt3AoYhJX7G"}},{"cell_type":"markdown","source":["### Aluno: João Antonio Costa Paiva Chagas"],"metadata":{"id":"X87NbdQ8JZmn"}},{"cell_type":"markdown","metadata":{"id":"hf0af9vLqLW3"},"source":["# Laboratório 8B: Q-learning (CliffWalking)"]},{"cell_type":"markdown","metadata":{"id":"f3ahVih93jpQ"},"source":["## Importações"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FTh_QK47EfwM"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import gymnasium as gym\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from typing import Dict, Tuple, List, Optional\n","from matplotlib.collections import LineCollection"]},{"cell_type":"markdown","source":["## Armazenamento"],"metadata":{"id":"ItBdrnoWTFNc"}},{"cell_type":"code","source":["output_dir = \"resultados_plots\"\n","os.makedirs(output_dir, exist_ok=True)"],"metadata":{"id":"_QmK9F3GTE_g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eLfMlW3VqLW5"},"source":["## Funções auxiliares para visualização"]},{"cell_type":"code","source":["def plotar_comparacao_metricas(\n","    results: Dict[str, Dict[str, list]],\n","    titulo_prefixo: str,\n","    janela: int = 100,\n","    save_path: Optional[str] = None\n",") -> None:\n","    \"\"\"\n","    Plota a comparação de métricas (tamanho e retorno) de múltiplos\n","    experimentos em uma única figura.\n","    \"\"\"\n","    fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12, 8), sharex=True)\n","    sns.set_palette(\"viridis\", n_colors=len(results)) # Optional: color palette\n","\n","    # Plot 1 – Tamanho do episódio\n","    for label, data in results.items():\n","        df = pd.DataFrame({'tamanho': data['T']})\n","        tamanho_ma = df['tamanho'].rolling(window=janela).mean()\n","        sns.lineplot(x=tamanho_ma.index, y=tamanho_ma, ax=axs[0], label=label)\n","\n","    axs[0].set_title(f'{titulo_prefixo} - Tamanho do Episódio')\n","    axs[0].set_ylabel(f'Passos por Episódio (Média Móvel {janela})')\n","    axs[0].legend()\n","    axs[0].grid()\n","\n","    # Plot 2 – Retorno\n","    for label, data in results.items():\n","        df = pd.DataFrame({'retorno': data['G']})\n","        retorno_ma = df['retorno'].rolling(window=janela).mean()\n","        sns.lineplot(x=retorno_ma.index, y=retorno_ma, ax=axs[1], label=label)\n","\n","    axs[1].set_title(f'{titulo_prefixo} - Retorno do Episódio')\n","    axs[1].set_xlabel('Episódio')\n","    axs[1].set_ylabel(f'Recompensa Total (Média Móvel {janela})')\n","    axs[1].legend()\n","    axs[1].grid()\n","\n","    plt.tight_layout()\n","    if save_path:\n","        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n","        plt.close(fig)\n","        print(f\"Plot de comparação salvo em: {save_path}\")\n","    else:\n","        plt.show()"],"metadata":{"id":"DnRwVcu2HGKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plotar_metricas_single(\n","    episodio_len: list[int],\n","    episodio_return: list[float],\n","    titulo: str,\n","    janela: int = 100,\n","    save_path: Optional[str] = None\n",") -> None:\n","    \"\"\"\n","    Usa Pandas + Seaborn para plotar as métricas de uma única execução.\n","    \"\"\"\n","    df = pd.DataFrame({\n","        'episodio': np.arange(len(episodio_len)),\n","        'tamanho': episodio_len,\n","        'retorno': episodio_return\n","    })\n","\n","    df['tamanho_ma'] = df['tamanho'].rolling(window=janela).mean()\n","    df['retorno_ma'] = df['retorno'].rolling(window=janela).mean()\n","\n","    fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(12, 8), sharex=True)\n","    fig.suptitle(titulo, fontsize=16)\n","\n","    # Plot 1 – Tamanho do episódio\n","    sns.lineplot(data=df, x='episodio', y='tamanho', ax=axs[0], label='Tamanho do Episódio', alpha=0.3)\n","    sns.lineplot(data=df, x='episodio', y='tamanho_ma', ax=axs[0], label=f'Média móvel ({janela})')\n","    axs[0].set_ylabel('Passos por Episódio')\n","    axs[0].legend()\n","    axs[0].grid()\n","\n","    # Plot 2 – Retorno\n","    sns.lineplot(data=df, x='episodio', y='retorno', ax=axs[1], label='Retorno do Episódio', color='orange', alpha=0.3)\n","    sns.lineplot(data=df, x='episodio', y='retorno_ma', ax=axs[1], label=f'Média móvel ({janela})', color='red')\n","    axs[1].set_xlabel('Episódio')\n","    axs[1].set_ylabel('Recompensa Total')\n","    axs[1].legend()\n","    axs[1].grid()\n","\n","    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make room for suptitle\n","    if save_path:\n","        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n","        plt.close(fig)\n","        print(f\"Plot de métricas salvo em: {save_path}\")\n","    else:\n","        plt.show()"],"metadata":{"id":"P-gzy6ksVrNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# helpers de grid/máscaras\n","def _grid_info_from_env(env_name: str, map_name: str | None = None, is_slippery: bool = False):\n","    \"\"\"\n","    Retorna (n_rows, n_cols, holes_set, cliffs_set, goals_set, start_rc)\n","    para FrozenLake-v1 ou CliffWalking-v1.\n","    \"\"\"\n","    holes, cliffs, goals = set(), set(), set()\n","    start_rc = None\n","\n","    if env_name == \"FrozenLake-v1\":\n","        kwargs = {}\n","        if map_name is not None:\n","            kwargs[\"map_name\"] = map_name\n","        kwargs[\"is_slippery\"] = is_slippery\n","\n","        env = gym.make(\"FrozenLake-v1\", **kwargs)\n","        desc = env.unwrapped.desc\n","        # decode se dtype for bytes\n","        desc = np.array(desc, dtype=str) if desc.dtype.kind != \"S\" else np.char.decode(desc, \"utf-8\")\n","        n_rows, n_cols = desc.shape\n","        for r in range(n_rows):\n","            for c in range(n_cols):\n","                ch = desc[r, c]\n","                if ch == \"H\": holes.add((r, c))\n","                elif ch == \"G\": goals.add((r, c))\n","                elif ch == \"S\": start_rc = (r, c)\n","        if start_rc is None:\n","            start_rc = (0, 0)\n","        env.close()\n","        return n_rows, n_cols, holes, cliffs, goals, start_rc\n","\n","    elif env_name == \"CliffWalking-v1\":\n","        n_rows, n_cols = 4, 12\n","        start_rc = (n_rows - 1, 0)\n","        goal_rc  = (n_rows - 1, n_cols - 1)\n","        goals.add(goal_rc)\n","        # penhasco: última linha, colunas 1..10\n","        for c in range(1, n_cols - 1):\n","            cliffs.add((n_rows - 1, c))\n","        return n_rows, n_cols, holes, cliffs, goals, start_rc\n","\n","    else:\n","        raise ValueError(\"env_name deve ser 'FrozenLake-v1' ou 'CliffWalking-v1'.\")\n","\n","def _rc_from_state(s: int, n_cols: int) -> tuple[int, int]:\n","    return divmod(s, n_cols)  # (r,c)\n","\n","# simulação de trajetória\n","def simular_trajetoria_gym(\n","    Pi: np.ndarray,\n","    env_name: str,\n","    *,\n","    map_name: str | None = None,\n","    is_slippery: bool = False,\n","    max_steps: int = 200\n","):\n","    \"\"\"\n","    Executa uma trajetória determinística (gulosa em Pi) no Gym (FrozenLake/CliffWalking).\n","    Retorna:\n","      - estados: lista de (r,c)\n","      - acoes: lista de ints\n","      - recompensas: lista de floats\n","    \"\"\"\n","    # cria env\n","    if env_name == \"FrozenLake-v1\":\n","        kwargs = {}\n","        if map_name is not None:\n","            kwargs[\"map_name\"] = map_name\n","        kwargs[\"is_slippery\"] = is_slippery\n","        env = gym.make(\"FrozenLake-v1\", **kwargs)\n","    elif env_name == \"CliffWalking-v1\":\n","        env = gym.make(\"CliffWalking-v1\")\n","    else:\n","        raise ValueError(\"env_name deve ser 'FrozenLake-v1' ou 'CliffWalking-v1'.\")\n","\n","    n_rows, n_cols, holes, cliffs, goals, start_rc = _grid_info_from_env(env_name, map_name, is_slippery)\n","\n","    # sanity check de dimensões\n","    n_states, n_actions = Pi.shape\n","    assert n_states == n_rows * n_cols, f\"Pi.shape[0]={n_states} != n_rows*n_cols={n_rows*n_cols}\"\n","    assert n_actions == env.action_space.n, f\"Pi.shape[1]={n_actions} != action_space.n={env.action_space.n}\"\n","\n","    state, _ = env.reset()\n","    # se o env permitir setar estado diretamente, tenta (FrozenLake/cliff costumam expor .unwrapped.s)\n","    if hasattr(env.unwrapped, \"s\"):\n","        env.unwrapped.s = int(state)\n","\n","    estados_rc = [_rc_from_state(int(state), n_cols)]\n","    acoes, recompensas = [], []\n","\n","    for _ in range(max_steps):\n","        s = int(state)\n","        a = int(np.argmax(Pi[s]))\n","        next_state, reward, terminated, truncated, _ = env.step(a)\n","\n","        acoes.append(a)\n","        recompensas.append(float(reward))\n","        estados_rc.append(_rc_from_state(int(next_state), n_cols))\n","\n","        state = next_state\n","        if terminated or truncated:\n","            break\n","\n","    env.close()\n","    return estados_rc, acoes, recompensas"],"metadata":{"id":"DABfiX7kK2ER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --------------------------\n","# plot da trajetória no grid\n","# --------------------------\n","def plot_trajetoria_gym(\n","    env_name: str,\n","    estados: list[tuple[int, int]],\n","    *,\n","    map_name: str | None = None,\n","    is_slippery: bool = False,\n","    titulo: str = \"Trajetória gulosa (Gym)\",\n","    gradiente_temporal: bool = True,\n","    mostrar_setas: bool = True,\n","    save_path: Optional[str] = None\n","):\n","    \"\"\"\n","    Plota a trajetória sobre um grid para FrozenLake/CliffWalking com destaques de buracos/penhasco/goal.\n","    \"\"\"\n","    n_rows, n_cols, holes, cliffs, goals, start_rc = _grid_info_from_env(env_name, map_name, is_slippery)\n","\n","    # figura/base do grid\n","    fig, ax = plt.subplots(figsize=(n_cols, n_rows))\n","    ax.set_xlim(0, n_cols); ax.set_ylim(0, n_rows)\n","    ax.set_xticks(np.arange(0, n_cols + 1, 1))\n","    ax.set_yticks(np.arange(0, n_rows + 1, 1))\n","    ax.grid(True); ax.set_aspect('equal'); ax.invert_yaxis()\n","\n","    # células coloridas\n","    for r in range(n_rows):\n","        for c in range(n_cols):\n","            cell = (r, c)\n","            if env_name == \"FrozenLake-v1\":\n","                if cell in holes:\n","                    color = (1.0, 0.0, 0.0, 0.25)  # vermelho translúcido\n","                elif cell in goals:\n","                    color = (0.0, 1.0, 0.0, 0.25)  # verde translúcido\n","                elif cell == start_rc:\n","                    color = (1.0, 1.0, 0.0, 0.18)  # amarelo leve\n","                else:\n","                    color = 'white'\n","            else:  # CliffWalking\n","                if cell in cliffs:\n","                    color = (1.0, 0.0, 0.0, 0.25)\n","                elif cell in goals:\n","                    color = (0.0, 1.0, 0.0, 0.25)\n","                elif cell == start_rc:\n","                    color = (1.0, 1.0, 0.0, 0.18)\n","                else:\n","                    color = 'white'\n","\n","            rect = patches.Rectangle((c, r), 1, 1, facecolor=color, edgecolor='gray')\n","            ax.add_patch(rect)\n","\n","    # extrai xs, ys (centros)\n","    xs = [c + 0.5 for (_, c) in estados]\n","    ys = [r + 0.5 for (r, _) in estados]\n","\n","    # linha com gradiente temporal\n","    if gradiente_temporal and len(xs) > 1:\n","        pontos = np.array([xs, ys]).T\n","        segmentos = np.stack([pontos[:-1], pontos[1:]], axis=1)\n","        lc = LineCollection(segmentos, linewidths=2.5)\n","        cores = np.linspace(0, 1, len(segmentos))\n","        lc.set_array(cores)\n","        ax.add_collection(lc)\n","        cbar = plt.colorbar(lc, ax=ax, fraction=0.046, pad=0.04)\n","        cbar.set_label(\"Progresso temporal\")\n","    else:\n","        ax.plot(xs, ys, '-o', linewidth=2.5, markersize=4)\n","\n","    # início/fim\n","    ax.scatter(xs[0], ys[0], marker='*', s=220, edgecolor='black', facecolor='yellow', zorder=5, linewidths=1.2, label='Início')\n","    ax.scatter(xs[-1], ys[-1], marker='o', s=150, edgecolor='black', facecolor='none', zorder=5, linewidths=1.2, label='Fim')\n","\n","    # setas entre células\n","    if mostrar_setas:\n","        for i in range(len(xs) - 1):\n","            dx, dy = xs[i+1] - xs[i], ys[i+1] - ys[i]\n","            ax.arrow(xs[i], ys[i], dx*0.85, dy*0.85, head_width=0.15, head_length=0.15,\n","                     length_includes_head=True, fc='black', ec='black', alpha=0.8)\n","\n","    ax.set_title(titulo)\n","    ax.legend(loc='upper right', frameon=True)\n","    plt.tight_layout()\n","    if save_path:\n","        directory = os.path.dirname(save_path)\n","        if directory:\n","            os.makedirs(directory, exist_ok=True)\n","        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n","        plt.close(fig)\n","        print(f\"Plot da política salvo em: {save_path}\")\n","    else:\n","        plt.show()"],"metadata":{"id":"6PpD6ldhF5C9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AOeAMZ4ux_GJ"},"source":["## Algoritmo: Q-learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-l5ZQkywifft"},"outputs":[],"source":["# Atualização dos valores de ação (Q-learning)\n","def _atualiza_Q(Q: np.ndarray, experiencia: Tuple[int,int,float,int], alpha: float, gamma: float) -> None:\n","    \"\"\"\n","    Atualização do Q-learning:\n","      Q(s,a) ← Q(s,a) - α [ Q(s,a) - ( r + γ max_{a'} Q(s', a') ) ]\n","    \"\"\"\n","    s, a, r, s_next = experiencia\n","    td_target = r + gamma * np.max(Q[s_next])\n","    td_error  = Q[s, a] - td_target\n","    Q[s, a]  -= alpha * td_error\n","\n","# Atualização da política ALVO\n","def _atualiza_Pi(Pi: np.ndarray, Q: np.ndarray, s: int) -> None:\n","    \"\"\"\n","    Torna Pi[s, :] gulosa em torno de argmax_a Q[s, a].\n","    \"\"\"\n","    a_star = int(np.argmax(Q[s]))\n","    Pi[s, :] = 0.0\n","    Pi[s, a_star] = 1.0\n","\n","# Política de COMPORTAMENTO ε-suave\n","def _eps_suave(Pi: np.ndarray, Q: np.ndarray, s: int, eps: float) -> None:\n","    \"\"\"\n","    Deixa Pi[s, :] ε-suave em torno de argmax_a Q[s, a] (para a política de COMPORTAMENTO).\n","    \"\"\"\n","    n_acoes = Q.shape[1]\n","    a_star = int(np.argmax(Q[s]))\n","    Pi[s, :] = eps / n_acoes\n","    Pi[s, a_star] += 1.0 - eps\n","\n","# Desempenho da POLÍTICA ALVO (gulosa, determinística)\n","def avaliar_desempenho(\n","    env,\n","    Pi: np.ndarray,\n","    max_steps: int = 200,\n",") -> Tuple[int, float]:\n","    \"\"\"\n","    Executa 1 episódio usando a política alvo (gulosa) e retorna (t, G) — passos e retorno não descontado.\n","    \"\"\"\n","    s, _ = env.reset()\n","    G, t = 0.0, 0\n","    while t < max_steps:\n","        a = int(np.argmax(Pi[s]))\n","        s, r, terminated, truncated, _ = env.step(a)\n","        G += r\n","        t += 1\n","        if terminated or truncated:\n","            break\n","    return t, G\n","\n","# Q-learning\n","def q_learning(\n","    env,\n","    gamma: float = 0.9,\n","    N: int = 500,\n","    T: int = 1_000,\n","    epsilon: float = 0.1,\n","    alpha: float = 0.1,\n","    seed: Optional[int] = None\n",") -> Tuple[np.ndarray, np.ndarray, np.ndarray, int, List[int], List[float]]:\n","    \"\"\"\n","    Q-learning com política de comportamento ε-gulosa (ε-suave) e política alvo gulosa.\n","\n","    - A política de comportamento (ε-gulosa) é utilizada para explorar o ambiente.\n","    - A política alvo é determinística e gulosa (greedy) em relação aos valores de ação Q.\n","    - A atualização de Q utiliza o melhor valor de ação possível em s' (independente da ação executada pela política de comportamento).\n","    - O desempenho é avaliado após cada episódio executando a política alvo de forma determinística.\n","\n","    Laço por episódio:\n","      1. Reinicia o ambiente: s0 ← env.reset().\n","      2. Seleciona a0 da política de comportamento: a0 ~ Pi_behavior(s0).\n","      3. Para cada passo:\n","         a. Executa a ação a_t no ambiente: observa (r_{t+1}, s_{t+1}, terminated, truncated).\n","         b. Atualiza Q(s_t, a_t):\n","            Q(s_t,a_t) ← Q(s_t,a_t) - α [Q(s_t,a_t) - (r_{t+1} + γ max_{a'} Q(s_{t+1},a'))].\n","         c. Atualiza a política alvo Pi_target(s_t) como gulosa em torno de argmax_a Q(s_t,a).\n","         d. Atualiza a política de comportamento Pi_behavior(s_t) para ser ε-suave em torno de argmax_a Q(s_t,a).\n","         e. Seleciona a_{t+1} da política de comportamento: a_{t+1} ~ Π_behavior(s_{t+1}).\n","         f. Avança o estado: s ← s_{t+1}, a ← a_{t+1}.\n","         g. Encerra o episódio se terminated ou truncated.\n","      4. Após cada episódio, executa a política alvo para medir o desempenho (retorno não descontado G e duração t).\n","\n","    Parâmetros\n","    ----------\n","    env : gymnasium.Env\n","        Ambiente Gymnasium.\n","    gamma : float\n","        Fator de desconto (0 ≤ γ ≤ 1).\n","    N : int\n","        Número total de episódios.\n","    epsilon : float\n","        Parâmetro ε da política de comportamento ε-gulosa (0 ≤ ε ≤ 1).\n","    alpha : float\n","        Taxa de aprendizado (0 < α ≤ 1).\n","    seed : int | None\n","        Semente para reprodutibilidade dos sorteios aleatórios.\n","\n","    Retorna\n","    -------\n","    Q : np.ndarray, shape (n_states, n_actions)\n","        Estimativas finais da função de ação Q(s,a).\n","    Pi_target : np.ndarray, shape (n_states, n_actions)\n","        Política alvo aprendida (determinística e gulosa em relação a Q).\n","    numero_de_visitas : np.ndarray, shape (n_states, n_actions)\n","        Contagem do número de visitas a cada par (s,a) durante o treinamento.\n","    N : int\n","        Número total de episódios efetivamente executados (igual ao parâmetro N).\n","    episodio_T : list[int]\n","        Lista contendo o número de passos de cada execução da política alvo após cada episódio.\n","    episodio_G : list[float]\n","        Lista contendo o retorno não descontado (soma das recompensas) da política alvo após cada episódio.\n","    \"\"\"\n","\n","    rng = np.random.default_rng(seed)\n","\n","    # Métricas por episódio\n","    episodio_T = []\n","    episodio_G = []\n","\n","    # Atalhos\n","    n_estados  = env.observation_space.n\n","    n_acoes = env.action_space.n\n","\n","    # Inicializações\n","    Q                 = np.zeros((n_estados, n_acoes), dtype=float)                 # Valores de ação\n","    numero_de_visitas = np.zeros((n_estados, n_acoes), dtype=float)                   # Número de visitas\n","    Pi_target         = np.zeros((n_estados, n_acoes), dtype=float)                   # Política alvo\n","    Pi_behavior       = np.full((n_estados, n_acoes), 1.0 / n_acoes, dtype=float)     # Política de comportamento uniforme\n","\n","    # Loop de episódios\n","    for _ in tqdm(range(1, N + 1), desc=\"Episódios (Q-learning)\", leave=True):\n","        s, _ = env.reset()\n","\n","        ############################################################################\n","        # Implementação aqui\n","        # Dica:\n","        # usar\n","        # próximo estado, recompensa, terminated (flag), truncated (flag), _ = env.step(ação)\n","        # para fazer a trasição de um estado para o outro dada uma ação do agente\n","\n","        for t in range(T):\n","\n","            # Atualiza a política de compartamento para o estado ATUAL 's'\n","            _eps_suave(Pi_behavior, Q, s, epsilon)\n","\n","            # Escolhe ação inicial a_0 ~ Pi(s_0)\n","            a = int(rng.choice(n_acoes, p=Pi_behavior[s]))\n","\n","            # Contagem de visitas ao par (s,a)\n","            numero_de_visitas[s, a] += 1.0\n","\n","            # Interage com o ambiente\n","            s_next, r, terminated, truncated, _ = env.step(a)\n","\n","            # Atualização de Q(s,a): Q-learning\n","            experiencia = (s, a, r, s_next)\n","            _atualiza_Q(Q, experiencia, alpha, gamma)\n","\n","            # Atualização da política alvo Pi_target para s\n","            _atualiza_Pi(Pi_target, Q, s)\n","\n","            # Avança o estado\n","            s = s_next\n","\n","            # Verifica se o episódio terminou\n","            if terminated or truncated:\n","                break\n","\n","        # Desempenho da política ALVO\n","        t, G = avaliar_desempenho(env, Pi_target, T)\n","        episodio_T.append(t)\n","        episodio_G.append(G)\n","\n","        ############################################################################\n","\n","    return Q, Pi_target, numero_de_visitas, N, episodio_T, episodio_G"]},{"cell_type":"markdown","metadata":{"id":"P2PS4lRgqLW9"},"source":["## Experimento"]},{"cell_type":"code","source":["def executar_experimento(\n","    nome_experimento: str,\n","    ambiente: str,\n","    algoritmo: callable,\n","    **kwargs\n",") -> Tuple[np.ndarray, List[int], List[float]]:\n","    \"\"\"\n","    Executa o algoritmo e retorna a política final, T e G.\n","    \"\"\"\n","    print(f\"--- Executando: {nome_experimento} ---\")\n","    env = gym.make(ambiente)\n","    try:\n","        _, Pi, _, _, T, G = algoritmo(env=env, **kwargs)\n","        return Pi, T, G\n","    finally:\n","        env.close()"],"metadata":{"id":"c-jaaL9iGmQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def executar_variacao_de_hiperparametro(\n","    nome_variacao: str,\n","    param_valores: list,\n","    algoritmo: callable,\n","    ambiente: str,\n","    output_dir: str,\n","    params_fixos: dict\n","):\n","    \"\"\"\n","    Executa variações de um hiperparâmetro,\n","    gerando plots comparativos de métricas e trajetórias individuais.\n","    \"\"\"\n","    print(f\"\\n==============================================\")\n","    print(f\"  Iniciando: Variação de {nome_variacao}\")\n","    print(f\"==============================================\\n\")\n","\n","    results_metricas = {}\n","    param_valores_sorted = sorted(list(param_valores))\n","\n","    # 1. Resultados de todas as execuções\n","    for valor in param_valores_sorted:\n","        nome_exp = f\"{nome_variacao}={valor}\"\n","        params_atuais = params_fixos.copy()\n","        chave_param = {\"EPISODIOS\": \"N\", \"ALPHA\": \"alpha\", \"GAMMA\": \"gamma\", \"EPSILON\": \"epsilon\"}[nome_variacao]\n","        params_atuais[chave_param] = valor\n","\n","        Pi, T, G = executar_experimento(\n","            nome_experimento=nome_exp,\n","            ambiente=ambiente,\n","            algoritmo=algoritmo,\n","            **params_atuais\n","        )\n","\n","        results_metricas[nome_exp] = {'T': T, 'G': G}\n","\n","        # 2. Plota a trajetória individual para cada execução\n","        estados, _, _ = simular_trajetoria_gym(Pi, ambiente, max_steps=200)\n","        plot_trajetoria_gym(\n","            ambiente,\n","            estados,\n","            titulo=f\"Trajetória (gulosa) — {nome_exp}\",\n","            save_path=f\"{output_dir}/trajetoria_{nome_exp}.pdf\"\n","        )\n","\n","    # 3. Plota comparação de métricas para a variação do hiperparâmetro\n","    plotar_comparacao_metricas(\n","        results=results_metricas,\n","        titulo_prefixo=f\"Variação de {nome_variacao}\",\n","        save_path=f\"{output_dir}/comparacao_{nome_variacao}.pdf\"\n","    )\n","    print(f\"\\n--- Variação de '{nome_variacao}' concluída. ---\")"],"metadata":{"id":"8xZhlLqdIAxK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"okxUUETkqLW_"},"source":["# Tarefa:\n","\n","1. Implemente o algoritmo Q-learning para resolver o ambiente `'CliffWalking-v1'` do [gymnasium](https://gymnasium.farama.org/environments/toy_text/cliff_walking/).\n","2. Considere os 4 hiperparametros (EPISODIOS, ALPHA, GAMMA, EPSILON)\n","    - Varie um dos hiperparametros (ex.: EPISODIOS) e fixe os demais (ex.: ALPHA, GAMMA, EPSILON). Obs.: 3 valores para cada hiperparâmetro.\n","    - Para cada estudo de hiperparâmetro plote:\n","        - a duração do episódio por episódio\n","        - a recompensa total por episodio\n","        - Uma trajetória gulosa utilizando `plot_trajetoria_gym`.\n","    - Observação: as curvas para cada estudo de hiperparâmetro devem estar na mesma figura, isto é, se o hiperparâmetro a ser variado é o EPSILON com 3 valores, entao o gráfico de duração do episódio deve mostrar as 4 curvas relativas a cada valor de EPSILON (com legenda) e de maneira similar para a recompensa total por episodio.\n","3. Repita o procedimento para cada um dos hiperparametros.\n","4. Reporte suas observações.\n","5. Compare com os resultados obtidos com o algoritmo SARSA.\n","\n","**Observação:**\n","- **Utilize os mesmos valores de hiperparâmetros do laboratório 7B (SARSA).**\n","\n","**Entregáveis:**\n","\n","2. **Código** (notebook `*.ipynb`)\n","1. **Relatório** (`*.pdf`).\n","- O PDF deve conter:\n","  - **Setup** (parâmetros usados).\n","  - **Resultados** (figuras e tabelas organizadas por experimento).\n","  - **Análises curtas** por experimento.\n","- O PDF **NÃO** deve conter:\n","    - Códigos."]},{"cell_type":"markdown","source":["## Hiperparâmetros Fixos"],"metadata":{"id":"2JZMB7-_M6QN"}},{"cell_type":"code","source":["params_fixos = {\n","    \"N\": 2000,\n","    \"T\": 1000,\n","    \"alpha\": 0.01,\n","    \"gamma\": 0.9,\n","    \"epsilon\": 0.3,\n","    \"seed\": 42\n","}"],"metadata":{"id":"c08Q8lq3M5wM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Baseline"],"metadata":{"id":"9FYY-ESaVzxc"}},{"cell_type":"code","source":["print(\"\\n################# EXPERIMENTO BASELINE #################\")\n","Pi_base, T_base, G_base = executar_experimento(\n","    nome_experimento=\"BASELINE\",\n","    ambiente=\"CliffWalking-v1\",\n","    algoritmo=q_learning,\n","    **params_fixos\n",")\n","\n","plotar_metricas_single(\n","    episodio_len=T_base,\n","    episodio_return=G_base,\n","    titulo=\"Métricas da Execução Baseline\",\n","    save_path=f\"{output_dir}/comparacao_BASELINE.pdf\"\n",")\n","\n","estados_base, _, _ = simular_trajetoria_gym(Pi_base, \"CliffWalking-v1\", max_steps=200)\n","plot_trajetoria_gym(\n","    \"CliffWalking-v1\",\n","    estados_base,\n","    titulo=\"Trajetória (gulosa) — BASELINE\",\n","    save_path=f\"{output_dir}/trajetoria_BASELINE.pdf\"\n",")"],"metadata":{"id":"Hu3ajxjV5kFE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Experimento 1 - Variação de EPISODIOS"],"metadata":{"id":"eZDjQEXeLrNB"}},{"cell_type":"code","source":["print(\"\\n################# EXPERIMENTO 1 #################\")\n","params_variacao_episodios = {k: v for k, v in params_fixos.items() if k != \"N\"}\n","executar_variacao_de_hiperparametro(\n","    nome_variacao=\"EPISODIOS\",\n","    param_valores=[1000, 1500, 2500, 3000],\n","    algoritmo=q_learning,\n","    ambiente=\"CliffWalking-v1\",\n","    output_dir=output_dir,\n","    params_fixos=params_variacao_episodios\n",")"],"metadata":{"id":"tjBeT1RRLlmj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Experimento 2 - Variação de ALPHA"],"metadata":{"id":"YYJ54aCFLteo"}},{"cell_type":"code","source":["print(\"\\n################# EXPERIMENTO 2 #################\")\n","params_variacao_alpha = {k: v for k, v in params_fixos.items() if k != \"alpha\"}\n","executar_variacao_de_hiperparametro(\n","    nome_variacao=\"ALPHA\",\n","    param_valores=[0.0001, 0.001, 0.05, 0.1],\n","    algoritmo=q_learning,\n","    ambiente=\"CliffWalking-v1\",\n","    output_dir=output_dir,\n","    params_fixos=params_variacao_alpha\n",")"],"metadata":{"id":"GDXf-HHsLwap"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Experimento 3 - Variação de GAMMA"],"metadata":{"id":"IvAaVx7-Ltz_"}},{"cell_type":"code","source":["print(\"\\n################# EXPERIMENTO 3 #################\")\n","params_variacao_gamma = {k: v for k, v in params_fixos.items() if k != \"gamma\"}\n","executar_variacao_de_hiperparametro(\n","    nome_variacao=\"GAMMA\",\n","    param_valores=[0.5, 0.7, 0.95, 0.99],\n","    algoritmo=q_learning,\n","    ambiente=\"CliffWalking-v1\",\n","    output_dir=output_dir,\n","    params_fixos=params_variacao_gamma\n",")"],"metadata":{"id":"J52A0N5ELw56"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Experimento 4 - Variação de EPSILON"],"metadata":{"id":"Csk0mApeLuJ4"}},{"cell_type":"code","source":["print(\"\\n################# EXPERIMENTO 4 #################\")\n","params_variacao_epsilon = {k: v for k, v in params_fixos.items() if k != \"epsilon\"}\n","executar_variacao_de_hiperparametro(\n","    nome_variacao=\"EPSILON\",\n","    param_valores=[0.1, 0.2, 0.6, 0.9],\n","    algoritmo=q_learning,\n","    ambiente=\"CliffWalking-v1\",\n","    output_dir=output_dir,\n","    params_fixos=params_variacao_epsilon\n",")"],"metadata":{"id":"cMif2JSrLxOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r /content/resultados_plots.zip /content/resultados_plots"],"metadata":{"id":"Ou6wX4SYPBqw"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["hf0af9vLqLW3","P2PS4lRgqLW9","okxUUETkqLW_","2JZMB7-_M6QN","eZDjQEXeLrNB","YYJ54aCFLteo","IvAaVx7-Ltz_"],"provenance":[{"file_id":"1mU4n7Ceywx-4cNTXvNbcv-tiZ73q9joW","timestamp":1761085671869}],"gpuType":"T4","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}