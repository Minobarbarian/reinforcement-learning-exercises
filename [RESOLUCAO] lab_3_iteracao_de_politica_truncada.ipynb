{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNE7OjzpND9rGe3N3zRYMSr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# IMD1103 - Aprendizado por Reforço"],"metadata":{"id":"7IlJowSa4Y6t"}},{"cell_type":"markdown","source":["### Professor: Dr. Leonardo Enzo Brito da Silva"],"metadata":{"id":"M5edkneO4dGw"}},{"cell_type":"markdown","source":["### Aluno: João Antonio Costa Paiva Chagas"],"metadata":{"id":"nYOnHe-yZvEi"}},{"cell_type":"markdown","metadata":{"id":"gYCGU2TiXK8x"},"source":["# Laboratório 3: Iteração de política truncada"]},{"cell_type":"markdown","metadata":{"id":"ekQmchLcXK8y"},"source":["## Importações"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtRypzNFEb3t"},"outputs":[],"source":["# In_estadostala os pacotes necessários:\n","# - gymnasium[toy-text]: inclui ambientes simples como FrozenLake, Taxi, etc.\n","# - imageio[ffmpeg]: permite salvar vídeos e GIFs (formato .mp4 ou .gif)\n","!pip install gymnasium[toy-text] imageio[ffmpeg]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FTh_QK47EfwM"},"outputs":[],"source":["# Importa as bibliotecas principais\n","import gymnasium as gym               # Biblioteca de simulações de ambientes para RL\n","import imageio                        # Usada para salvar a sequência de frames como GIF\n","from IPython.display import Image     # Para exibir a imagem (GIF) diretamente no notebook\n","import numpy as np                    # Importa o pacote NumPy, amplamente utilizado para manipulação de arrays e operações numéricas\n","from numpy import linalg as LA        # Rotinas de álgebra linear do NumPy (ex.: normas, autovalores, decomposições)\n","import matplotlib.pyplot as plt       # Biblioteca para criação de gráficos estáticos em Python (parte do matplotlib)\n","import seaborn as sns                 # Biblioteca baseada em matplotlib para gráficos estatísticos com visualização mais bonita (usada aqui para heatmaps)\n","from typing import Dict, Tuple, Optional, List  # Importa ferramentas de tipagem estática do Python\n","import os"]},{"cell_type":"markdown","source":["## Armazenamento"],"metadata":{"id":"ItBdrnoWTFNc"}},{"cell_type":"code","source":["output_dir = \"resultados_plots\"\n","os.makedirs(output_dir, exist_ok=True)"],"metadata":{"id":"_QmK9F3GTE_g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cZzYmyGzXK80"},"source":["## Funções auxiliares para visualização"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yc1S2u3YXK80"},"outputs":[],"source":["def visualizar_politica(\n","    Pi: np.ndarray,\n","    env,\n","    *,\n","    action_labels: Optional[List[str]] = None,   # default FrozenLake: [\"←\",\"↓\",\"→\",\"↑\"]\n","    destacar_gulosa: bool = True,\n","    suptitle: Optional[str] = \"Política (distribuições por estado)\",\n","    save_path: Optional[str] = None\n",") -> None:\n","    # Inferir grid do Gymnasium (FrozenLake)\n","    if not (hasattr(env, \"unwrapped\") and hasattr(env.unwrapped, \"desc\")):\n","        raise ValueError(\"Passe um ambiente Gymnasium com 'env.unwrapped.desc' (ex.: FrozenLake-v1).\")\n","    desc = env.unwrapped.desc\n","    desc_str = np.char.decode(desc, \"utf-8\") if getattr(desc.dtype, \"kind\", \"\") == \"S\" else desc.astype(str)\n","\n","    n_rows, n_cols = desc_str.shape\n","    n_estados, n_acoes = Pi.shape\n","    if n_rows * n_cols != n_estados:\n","        raise ValueError(f\"Incompatibilidade: grid {n_rows}x{n_cols} != n_estados={n_estados}.\")\n","\n","    # Máscaras de terminais\n","    holes = (desc_str == \"H\")\n","    goal  = (desc_str == \"G\")\n","\n","    # Rótulos das ações (FrozenLake: LEFT, DOWN, RIGHT, UP)\n","    if action_labels is None:\n","        action_labels = [\"←\", \"↓\", \"→\", \"↑\"]\n","    if len(action_labels) != n_acoes:\n","        action_labels = [f\"a{i}\" for i in range(n_acoes)]\n","\n","    # Figura\n","    fig, axs = plt.subplots(n_rows, n_cols, figsize=(3.0 * n_cols, 2.2 * n_rows))\n","    axs = np.array(axs).reshape(-1)\n","\n","    for s in range(n_estados):\n","        r, c = divmod(s, n_cols)\n","        ax = axs[s]\n","\n","        # Estados terminais: só fundo colorido, sem barras\n","        if holes[r, c] or goal[r, c]:\n","            if holes[r, c]:\n","                ax.set_facecolor((1.0, 0.0, 0.0, 0.15))  # vermelho translúcido\n","                ax.set_title(f\"Estado {s} (H)\")\n","            else:\n","                ax.set_facecolor((0.0, 1.0, 0.0, 0.15))  # verde translúcido\n","                ax.set_title(f\"Estado {s} (G)\")\n","            # esconder eixos e ticks\n","            ax.set_xticks([]); ax.set_yticks([])\n","            for spine in ax.spines.values():\n","                spine.set_visible(True)\n","            continue\n","\n","        # Estados não-terminais: barras\n","        pi = Pi[s].astype(float)\n","        tot = pi.sum()\n","        if tot > 0:\n","            pi /= tot  # normalização defensiva\n","\n","        acoes = np.arange(n_acoes)\n","        colors = [\"gray\"] * n_acoes\n","        if destacar_gulosa:\n","            colors[int(np.argmax(pi))] = \"dimgray\"\n","\n","        ax.bar(acoes, pi, color=colors)\n","        ax.set_ylim(0, 1.05)\n","        ax.set_xticks(acoes)\n","        ax.set_xticklabels(action_labels)\n","        ax.set_yticks([0, 0.5, 1.0])\n","        ax.set_title(f\"Estado {s}\")\n","\n","    if suptitle:\n","        fig.suptitle(suptitle, y=1.02, fontsize=12)\n","\n","    if fig is not None:\n","        plt.tight_layout()\n","        if save_path:\n","            directory = os.path.dirname(save_path)\n","            if directory:\n","                os.makedirs(directory, exist_ok=True)\n","            fig.savefig(save_path, dpi=300, bbox_inches='tight')\n","            plt.close(fig)\n","            print(f\"Plot da política salvo em: {save_path}\")\n","    else:\n","        plt.show()\n","\n","def plot_tabular(\n","    data,\n","    kind: str = \"Q\",          # \"Q\" (valores de ação), \"Pi\" (política), \"V\" (valores de estado)\n","    ambiente=None,            # necessário quando kind=\"V\" para reshape\n","    ax=None,\n","    cbar: bool = True,\n","    fmt: str = \".1f\",\n","    center_zero: bool = True,  # só relevante para \"Q\" e \"V\"\n","    save_path: Optional[str] = None\n","):\n","    \"\"\"\n","    Plota matrizes tabulares de RL em formato de heatmaps (mapas de calor).\n","    Esta função cobre 3 casos:\n","    1. kind=\"Q\": heatmap de Q(s, a) com ações nas linhas e estados nas colunas.\n","    2. kind=\"Pi\": heatmap de Pi(a|s) (probabilidades) com ações nas linhas e estados nas colunas.\n","    3. kind=\"V\": heatmap de V(s) no grid (n_rows x n_cols) do ambiente .\n","\n","    Parameters\n","    ----------\n","    data : ndarray\n","        Dados a serem plotados.\n","        - Para kind=\"Q\" ou \"Pi\": array 2D com shape (n_estados, n_acoes).\n","        - Para kind=\"V\": array 1D com shape (n_estados,) que será remodelado para (ambiente.n_rows, ambiente.n_cols).\n","    kind : {\"Q\", \"Pi\", \"V\"}, default=\"Q\"\n","        Tipo do plot:\n","        - \"Q\" usa paleta divergente centrada em zero.\n","        - \"Pi\" usa paleta sequencial no intervalo [0, 1].\n","        - \"V\" plota o valor de estado no grid do ambiente.\n","    ambiente : object, optional\n","        Necessário quando kind=\"V\". Deve expor n_rows e n_cols para o reshape.\n","    ax : matplotlib.axes.Axes, optional\n","        Eixo onde o heatmap será desenhado. Se None, uma nova figura/eixo é criado.\n","    cbar : bool, default=True\n","        Se True, exibe a barra de cores (colorbar).\n","    fmt : str, default=\".1f\"\n","        Formatação dos valores anotados em cada célula do heatmap.\n","    center_zero : bool, default=True\n","        Quando kind é \"Q\" ou \"V\", centraliza a escala de cores em zero (vmin=-absmax, vmax=absmax). Ignorado para \"Pi\".\n","\n","    Returns\n","    -------\n","    ax : matplotlib.axes.Axes\n","        Eixo contendo o heatmap resultante.\n","    \"\"\"\n","    kind = kind.upper()\n","\n","    xlabel = {\"V\": \"Colunas\", \"PI\": \"Estados\", \"Q\": \"Estados\"}\n","    ylabel = {\"V\": \"Linhas\", \"PI\": \"Ações\", \"Q\": \"Ações\" }\n","    title  = {\"V\": \"Valores de Estado (V(s))\", \"PI\": r\"Política ($\\pi(a|s)$ transposta)\", \"Q\": \"Valores de ação (Q(s, a) transposta)\"}\n","\n","    fig = None\n","\n","    #  V(s): precisa do shape do grid\n","    match kind:\n","        case \"V\":\n","\n","            if ambiente is None:\n","                raise ValueError(\"Para kind='V', passe 'ambiente' para reshape (n_rows, n_cols).\")\n","\n","            if hasattr(ambiente, \"n_rows\") and hasattr(ambiente, \"n_cols\"):\n","                n_rows, n_cols = ambiente.n_rows, ambiente.n_cols\n","            elif hasattr(ambiente, \"unwrapped\") and hasattr(ambiente.unwrapped, \"desc\"):\n","                # ex.: FrozenLake-v1 (Gymnasium)\n","                n_rows, n_cols = ambiente.unwrapped.desc.shape\n","            else:\n","                raise ValueError(\n","                    \"Passe um objeto com n_rows/n_cols ou um env Gymnasium com .unwrapped.desc.\"\n","                )\n","\n","            M = data.reshape(n_rows, n_cols)\n","\n","            if ax is None:\n","                fig, ax = plt.subplots(figsize=(n_cols, n_rows))\n","\n","            if center_zero:\n","                vmax = float(np.abs(M).max())\n","                vmin = -vmax\n","            else:\n","                vmin = float(M.min())\n","                vmax = float(M.max())\n","\n","            cmap, square = \"bwr\", True\n","\n","        case \"PI\" | \"Q\":\n","\n","            # Q(s,a) e Pi(a|s): ações nas linhas, estados nas colunas\n","            M = data.T  # data: (n_estados, n_acoes) -> transposto para (n_acoes, n_estados)\n","            n_acoes, n_estados = M.shape\n","\n","            if ax is None:\n","                fig, ax = plt.subplots(figsize=(n_estados, n_acoes))\n","\n","            if kind == \"PI\":\n","                cmap = \"Blues\";\n","                vmin, vmax = 0.0, 1.0\n","            else:  # \"Q\"\n","                cmap = \"bwr\"\n","                if center_zero:\n","                    vmax = float(np.abs(M).max())\n","                    vmin = -vmax\n","                else:\n","                    vmin = float(M.min())\n","                    vmax = float(M.max())\n","\n","            square = False\n","\n","        case _:\n","            raise ValueError(f\"kind desconhecido: {kind!r} (use 'Q', 'Pi' ou 'V').\")\n","\n","\n","    ax = sns.heatmap(\n","        data=M,\n","        annot=True,\n","        fmt=fmt,\n","        cmap=cmap,\n","        vmin=vmin,\n","        vmax=vmax,\n","        cbar=cbar,\n","        square=square,\n","        linewidths=0.5,\n","        linecolor=\"gray\",\n","        ax=ax\n","    )\n","\n","    ax.set_xlabel(xlabel[kind])\n","    ax.set_ylabel(ylabel[kind])\n","    ax.set_title(title[kind])\n","\n","    # bordas externas\n","    for side in (\"left\", \"right\", \"top\", \"bottom\"):\n","        ax.spines[side].set_visible(True)\n","        ax.spines[side].set_linewidth(0.5)\n","        ax.spines[side].set_edgecolor(\"gray\")\n","\n","    # rótulos\n","    if kind in (\"Q\", \"PI\"):\n","        ax.set_xticks(np.arange(n_estados) + 0.5)\n","        ax.set_xticklabels([f\"s{i}\" for i in range(n_estados)], rotation=0)\n","        ax.set_yticks(np.arange(n_acoes) + 0.5)\n","        ax.set_yticklabels([f\"a{i}\" for i in range(n_acoes)], rotation=0)\n","\n","    if fig is not None:\n","      plt.tight_layout()\n","      if save_path:\n","          directory = os.path.dirname(save_path)\n","          if directory:\n","              os.makedirs(directory, exist_ok=True)\n","          fig.savefig(save_path, dpi=300, bbox_inches='tight')\n","          plt.close(fig)\n","          print(f\"Plot tabular '{kind}' salvo em: {save_path}\")\n","      else:\n","          plt.show()\n","\n","    return"]},{"cell_type":"code","source":["def plotar_grafico_dispersao(\n","    j_truncado_lista: List[int],\n","    iteracoes_k: List[int],\n","    map_name: str,\n","    is_slippery: bool,\n","    save_path: Optional[str] = None\n",") -> None:\n","    \"\"\"\n","    Gera um gráfico de dispersão mostrando o número de iterações (k) para\n","    convergir em função do número de varreduras de avaliação (j_truncado).\n","\n","    Parâmetros\n","    ----------\n","    j_truncado_lista : List[int]\n","        Valores de j_truncado testados (eixo x).\n","    iteracoes_k : List[int]\n","        Número de iterações k correspondente a cada j_truncado (eixo y).\n","    map_name : str\n","        Nome do mapa do ambiente (usado no título).\n","    is_slippery : bool\n","        Se o ambiente é estocástico (usado no título).\n","    \"\"\"\n","    plt.figure(figsize=(10, 6))\n","    plt.scatter(j_truncado_lista, iteracoes_k, marker='o', s=80, alpha=0.8, label='Dados do Experimento')\n","    plt.plot(j_truncado_lista, iteracoes_k, linestyle='--', alpha=0.5, label='Linha de Tendência')\n","\n","    plt.title(f'Convergência do Algoritmo vs. j_truncado (Mapa: {map_name}, Slippery: {is_slippery})', fontsize=14)\n","    plt.xlabel('j_truncado (Varreduras de Avaliação por Iteração)', fontsize=12)\n","    plt.ylabel('Iterações Externas para Convergência (k)', fontsize=12)\n","    plt.xticks(j_truncado_lista)\n","    plt.legend()\n","    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n","\n","    if save_path:\n","        plt.tight_layout()\n","        directory = os.path.dirname(save_path)\n","        if directory:\n","            os.makedirs(directory, exist_ok=True)\n","        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n","        plt.close()\n","        print(f\"Gráfico de dispersão salvo em: {save_path}\")\n","    else:\n","        plt.show()"],"metadata":{"id":"oyVQb2InGuWJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ncWDixZ6XK81"},"source":["## Algoritmo: Iteração de política truncada"]},{"cell_type":"markdown","metadata":{"id":"iKzZGMreXK81"},"source":["### Avaliar política truncada"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPpOKQMeXK81"},"outputs":[],"source":["def avaliar_politica_truncada(\n","    env,\n","    Pi: np.ndarray,\n","    j_truncado: int,\n","    V: np.ndarray | None = None,\n","    gamma: float = 0.9,\n",") -> np.ndarray:\n","    \"\"\"\n","    Avaliação de política truncada.\n","\n","    Executa exatamente j_truncado varreduras do algoritmo de avaliação de política (Jacobi).\n","\n","    Atualização:\n","        V_{new}(s) = sum_a Pi[a|s] * sum_{(p,s',r,done) in env.P[s][a]} p * [ r + gamma * V_old[s'] ]\n","\n","    Parâmetros\n","    ----------\n","    env : gym.Env (unwrapped)\n","        Ambiente com dicionário de transições env.P: Dict[s][a] -> List[(p, s', r, done)].\n","    Pi : np.ndarray, shape (n_estados, n_acoes)\n","        Política.\n","    j_truncado : int\n","        Número de iterações (varreduras de avaliação).\n","    V : np.ndarray | None, shape (n_estados,), opcional\n","        Valores de estado iniciais. Se None, começa em zeros.\n","    gamma : float, default=0.9\n","        Fator de desconto.\n","\n","    Retorna\n","    -------\n","    V : np.ndarray, shape (n_estados,)\n","        Valores de estado após j_truncado varreduras.\n","    \"\"\"\n","\n","    n_estados = env.observation_space.n\n","    n_acoes   = env.action_space.n\n","\n","    # Inicializações\n","    if V is None:\n","        V = np.zeros(n_estados, dtype=float)\n","\n","    ############################################################################################################\n","    # AVALIAÇÃO DA POLÍTICA ATUAL\n","    ############################################################################################################\n","    # Código aqui\n","\n","    # V_atual representa o vetor de valores V que será atualizado a cada iteração interna\n","    # Começa como uma cópia do V da iteração de política anterior\n","    V_atual = V.copy()\n","\n","    # Executa um número fixo de varreduras (j_truncado) para avaliar a política\n","    for _ in range(j_truncado):\n","        # V_proximo armazenará os novos valores calculados nesta varredura.\n","        V_proximo = np.zeros(n_estados, dtype=float)\n","\n","        # Itera sobre cada estado para calcular seu novo valor.\n","        for estado in range(n_estados):\n","            valor_estado = 0\n","\n","            # Itera sobre cada ação possível a partir do estado.\n","            # Pi[estado] é a distribuição de probabilidade das ações para o estado atual.\n","            for acao, prob_acao in enumerate(Pi[estado]):\n","\n","                # Calcula o valor apenas para ações com probabilidade > 0.\n","                if prob_acao > 0:\n","                    valor_acao_esperado = 0\n","\n","                    # Calcula o valor esperado de tomar a ação no estado.\n","                    for prob, proximo_estado, recompensa, _ in env.P[estado][acao]:\n","                        valor_acao_esperado += prob * (recompensa + gamma * V_atual[proximo_estado])\n","\n","                    # Pondera o valor esperado da ação pela probabilidade de escolhê-la.\n","                    valor_estado += prob_acao * valor_acao_esperado\n","\n","            # Atribui o valor calculado para o estado no vetor da próxima iteração.\n","            V_proximo[estado] = valor_estado\n","\n","        # Atualiza V_atual com os valores recém-calculados para a próxima varredura.\n","        V_atual = V_proximo.copy()\n","\n","    ############################################################################################################\n","\n","    return V_atual\n"]},{"cell_type":"markdown","metadata":{"id":"CX7YJUT-XK82"},"source":["### Melhorar política truncada"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qf_3OcrFXK82"},"outputs":[],"source":["def melhorar_politica(\n","    env,\n","    V: np.ndarray,\n","    gamma: float = 0.9,\n",") -> Tuple[np.ndarray, np.ndarray]:\n","    \"\"\"\n","    Melhoria de política.\n","\n","    Dado V, calcula:\n","        Q(s,a) = sum_{(p,s',r,done) in P[s][a]} p * (r + gamma * V[s'])\n","    e retorna a política gulosa determinística.\n","\n","    Parâmetros\n","    ----------\n","    env : gym.Env (unwrapped)\n","        Ambiente com dicionário de transições env.P: Dict[s][a] -> List[(p, s', r, done)].\n","    V : np.ndarray, shape (n_estados,)\n","        Valores de estado.\n","    gamma : float, default=0.99\n","        Fator de desconto (0 <= gamma < 1).\n","\n","    Retorna\n","    -------\n","    Q : np.ndarray, shape (n_estados, n_acoes)\n","        Valores de ação.\n","    Pi_nova : np.ndarray, shape (n_estados, n_acoes)\n","        Política gulosa determinística.\n","    \"\"\"\n","    # Dimensões e validações\n","\n","    n_estados = env.observation_space.n\n","    n_acoes   = env.action_space.n\n","\n","    # Inicializações\n","    Q       = np.zeros((n_estados, n_acoes), dtype=float)\n","    Pi_nova = np.zeros((n_estados, n_acoes), dtype=float)\n","\n","    ############################################################################################################\n","    # MELHORIA DA POLÍTICA\n","    ############################################################################################################\n","    # Código aqui\n","\n","    # Itera sobre cada estado para calcular os valores de Q e determinar a melhor ação\n","    for estado in range(n_estados):\n","        # Calcula o valor Q para cada ação possível no estado atual\n","        for acao in range(n_acoes):\n","            valor_acao_esperado = 0\n","\n","            # Soma sobre todos os possíveis resultados (s', r) para o par (estado, acao)\n","            for prob, proximo_estado, recompensa, _ in env.P[estado][acao]:\n","                valor_acao_esperado  += prob * (recompensa + gamma * V[proximo_estado])\n","\n","            Q[estado, acao] = valor_acao_esperado\n","\n","        # Após calcular Q(estado, acao) para todas as ações em determinado estado, encontra a melhor ação (aquela com o maior valor Q)\n","        melhor_acao = np.argmax(Q[estado])\n","\n","        # Cria uma política determinística que sempre escolhe a melhor ação\n","        Pi_nova[estado, melhor_acao] = 1.0\n","    ############################################################################################################\n","\n","    return Q, Pi_nova"]},{"cell_type":"markdown","metadata":{"id":"IJPj11kAXK82"},"source":["### Algoritmo: iteração de política truncada"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BvaiYe-RXK82"},"outputs":[],"source":["def iteracao_de_politica_truncada(\n","    env,\n","    gamma: float = 0.99,\n","    j_truncado: int = 10,\n","    theta: float = 1e-8,\n",") -> Tuple[np.ndarray, np.ndarray, np.ndarray, int]:\n","    \"\"\"\n","    Iteração de Política truncada.\n","\n","    Loop externo (k):\n","      1) V <- avaliar_politica_truncada(env, Pi_k, j_truncado, V, gamma)\n","      2) (Q, Pi_{k+1}) <- melhorar_politica(env, V, gamma)\n","      3) parar quando ||V - V_prev||_inf < theta\n","\n","    Parâmetros\n","    ----------\n","    env : gym.Env (unwrapped)\n","        Ambiente com dicionário de transições env.P: Dict[s][a] -> List[(p, s', r, done)].\n","    gamma : float, default=0.9\n","        Fator de desconto (0 <= gamma < 1).\n","    j_truncado : int, default=10\n","        Número de varreduras na avaliação de política truncada.\n","    theta : float, default=1e-8\n","        Critério de parada baseado em V entre iterações externas (convergência).\n","\n","    Retorna\n","    -------\n","    V  : np.ndarray, shape (n_estados,)\n","        Valores de estado.\n","    Q  : np.ndarray, shape (n_estados, n_acoes)\n","        Valores de ação da última melhoria.\n","    Pi : np.ndarray, shape (n_estados, n_acoes)\n","        Política determinística.\n","    k  : int\n","        Número de iterações externas executadas.\n","    \"\"\"\n","    # Dimensões e validações básicas\n","    n_estados = env.observation_space.n\n","    n_acoes   = env.action_space.n\n","\n","    # Inicializações\n","    V  = np.zeros(n_estados, dtype=float)\n","    Q  = np.zeros((n_estados, n_acoes), dtype=float)\n","    Pi = np.full((n_estados, n_acoes), 1.0 / n_acoes, dtype=float)  # política uniforme\n","\n","    # Laço externo\n","    k = 0\n","    while True:\n","\n","        k += 1\n","\n","        V_prev = V.copy()\n","\n","        # 1) Avaliação de política (truncada)\n","        V = avaliar_politica_truncada(\n","            env=env,\n","            Pi=Pi,\n","            j_truncado=j_truncado,\n","            V=V,\n","            gamma=gamma,\n","        )\n","\n","        # 2) Melhoria de política (gulosa)\n","        Q, Pi_nova = melhorar_politica(\n","            env=env,\n","            V=V,\n","            gamma=gamma,\n","        )\n","        Pi = Pi_nova\n","\n","        # 3) Critério de parada baseado apenas em V\n","        if np.linalg.norm(V - V_prev, ord=np.inf) < theta:\n","            break\n","\n","    return V, Q, Pi, k\n"]},{"cell_type":"markdown","source":["## Experimento"],"metadata":{"id":"By0U9xtFH5pe"}},{"cell_type":"code","source":["def executar_e_plotar_experimento_truncado(\n","    nome_experimento: str,\n","    env,\n","    output_dir: str,\n","    gamma: float,\n","    j_truncado: int,\n","    theta: float\n","):\n","    \"\"\"\n","    Executa a Iteração de Política Truncada para uma configuração específica,\n","    imprime a convergência e salva todas as figuras de visualização.\n","    \"\"\"\n","    print(f\"\\n==============================================\")\n","    print(f\"  Executando: {nome_experimento}\")\n","    print(f\"==============================================\\n\")\n","\n","    # Roda o algoritmo\n","    V, Q, Pi, k = iteracao_de_politica_truncada(\n","        env,\n","        gamma=gamma,\n","        j_truncado=j_truncado,\n","        theta=theta\n","    )\n","\n","    print(f\"--> Convergência em {k} iterações para j_truncado={j_truncado}.\\n\")\n","\n","    # Gera e salva todas as figuras\n","    print(\"Salvando figuras...\")\n","    plot_tabular(\n","        V, kind=\"V\", ambiente=env, center_zero=False,\n","        save_path=f\"{output_dir}/{nome_experimento}_V.pdf\"\n","    )\n","    plot_tabular(\n","        Q, kind=\"Q\",\n","        save_path=f\"{output_dir}/{nome_experimento}_Q.pdf\"\n","    )\n","    plot_tabular(\n","        Pi, kind=\"Pi\",\n","        save_path=f\"{output_dir}/{nome_experimento}_Pi.pdf\"\n","    )\n","    visualizar_politica(\n","        Pi, env=env, suptitle=f\"Política - {nome_experimento}\",\n","        save_path=f\"{output_dir}/{nome_experimento}_Politica.pdf\"\n","    )\n","    print(\"Figuras salvas!\")"],"metadata":{"id":"VrDLcCCTH46M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Função para Executar os Experimentos\n","\n","def executar_experimento_convergencia(\n","    map_name: str,\n","    is_slippery: bool,\n","    j_truncado_lista: List[int],\n","    gamma: float,\n","    theta: float,\n","    save_path: str\n",") -> None:\n","    \"\"\"\n","\n","    Executa o algoritmo de iteração de política truncada para diferentes\n","\n","    valores de j_truncado e plota um gráfico de dispersão dos resultados.\n","\n","    \"\"\"\n","\n","    env = gym.make(\"FrozenLake-v1\", map_name=map_name, is_slippery=is_slippery)\n","    env = env.unwrapped\n","    iteracoes_k = []\n","\n","    print(f\"--- Iniciando experimento para o mapa {map_name} ---\")\n","\n","    for j in j_truncado_lista:\n","        _, _, _, k = iteracao_de_politica_truncada(env, gamma=gamma, j_truncado=j, theta=theta)\n","        iteracoes_k.append(k)\n","\n","        print(f\"j_truncado = {j:3d} -> k = {k:3d} iterações para convergir\")\n","\n","    print(\"--- Experimento finalizado ---\")\n","\n","    # Chama a função de plotagem com os dados reais coletados\n","    plotar_grafico_dispersao(\n","        j_truncado_lista=j_truncado_lista,\n","        iteracoes_k=iteracoes_k,\n","        map_name=map_name,\n","        is_slippery=is_slippery,\n","        save_path=save_path\n","    )"],"metadata":{"id":"5pgvflXwtgjA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tarefa\n","\n","1. Implemente o algoritmo **iteração de política truncada**.\n","2. Gere um **gráfico de dispersão** em que cada ponto (x,y) corresponde à (valor do j_truncado, iteração em que a condição de convergência foi satisfeita para este j_truncado).\n","\n","** Utilize a seguinte configuração do ambiente FrozenLake para os experimentos**\n","\n","- `map_name = '8x8'` e `map_name = '4x4'`      \n","- `render_mode=\"rgb_array\"`\n","- `is_slippery=True`\n","\n","**No experimento com configuração `map_name = '4x4'` mostrar:**\n","\n","1. **Figuras**:\n","   - heatmap de $V(s)$ (função `plot_tabular`);\n","   - heatmap de $Q(s,a)$ (função `plot_tabular`);\n","   - heatmap de $\\pi(a\\mid s)$ (função `plot_tabular`);\n","   - gráficos de barras de $\\pi(a\\mid s)$ (função `visualizar_politica`).\n","   - gráfico de dispersão\n","\n","**No experimento com configuração `map_name = '8x8'` mostrar:**\n","\n","1. **Figura**:\n","   - gráfico de dispersão\n","\n","**Entregáveis:**\n","\n","2. **Código** (notebook `.ipynb`)\n","1. **Relatório** (`.pdf`).\n","- O PDF deve conter:\n","  - **Setup** (parâmetros usados).\n","  - **Resultados** (figuras e tabelas organizadas por experimento).\n","  - **Análises curtas** por experimento.\n","- O PDF **NÃO** deve conter:\n","    - Códigos."],"metadata":{"id":"SR7T2IbYZmsM"}},{"cell_type":"markdown","source":["### 2:"],"metadata":{"id":"pv4GoG6Z6AY1"}},{"cell_type":"code","source":["print(\"\\n################# TAREFA 2 #################\")"],"metadata":{"id":"4-rlldFcoDzi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["GAMMA = 0.95\n","THETA = 1e-8\n","IS_SLIPPERY = True\n","J_TRUNCADO_LISTA = [1, 2, 3, 5, 10, 15, 20, 50, 100]"],"metadata":{"id":"P1VcLVy7ICmz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 4x4:"],"metadata":{"id":"-kNZzAsJ5wz9"}},{"cell_type":"code","source":["print(\"\\n################# EXPERIMENTO MAPA 4x4 #################\")\n","\n","env_4x4 = gym.make(\"FrozenLake-v1\", map_name='4x4', is_slippery=IS_SLIPPERY)\n","env_4x4 = env_4x4.unwrapped\n","\n","executar_e_plotar_experimento_truncado(\n","    nome_experimento=\"4x4_j_truncado_20\",\n","    env=env_4x4,\n","    output_dir=output_dir,\n","    gamma=GAMMA,\n","    j_truncado=20,\n","    theta=THETA\n",")\n","\n","executar_experimento_convergencia(\n","    map_name='4x4',\n","    is_slippery=IS_SLIPPERY,\n","    j_truncado_lista=J_TRUNCADO_LISTA,\n","    gamma=GAMMA,\n","    theta=THETA,\n","    save_path=f\"{output_dir}/4x4_convergencia.pdf\"\n",")"],"metadata":{"id":"22h3g1LzOvmu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 8x8:"],"metadata":{"id":"AnWTkLtG57hm"}},{"cell_type":"code","source":["print(\"\\n################# EXPERIMENTO MAPA 8x8 #################\")\n","\n","executar_experimento_convergencia(\n","    map_name='8x8',\n","    is_slippery=IS_SLIPPERY,\n","    j_truncado_lista=J_TRUNCADO_LISTA,\n","    gamma=GAMMA,\n","    theta=THETA,\n","    save_path=f\"{output_dir}/8x8_convergencia.pdf\"\n",")"],"metadata":{"id":"pP8yIyAC5vya"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r /content/resultados_plots.zip {output_dir}"],"metadata":{"id":"gF0y6lD6Re80"},"execution_count":null,"outputs":[]}]}